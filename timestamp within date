# Databricks notebook source
# DBTITLE 1,Creating the sparkSession
import pandas as pd
from pyspark.sql import SparkSession
sc = SparkSession.builder.appName("1 count with datess").getOrCreate()


# COMMAND ----------

list1=[]
for i in range(1,21):
  dates= '01/'+str(i)+'/2019'
  list1.append(dates)
  print(dates)

dateList=['a','b','c','d','a','b','d','e','g','t','a','b','d','c','t','j','k','a','m','l']
len(dateList)

# COMMAND ----------


data = {"user": dateList,
         "date":list1}

df=pd.DataFrame(data)
df['date']= pd.to_datetime(df['date'])


# COMMAND ----------

# DBTITLE 1,test dataframe , we will create new one from this.
from pyspark.sql import *
#abc= [dateList, list]
#dataframe = Row(user=dateList)
#display(dataframe)

dfs=sc.createDataFrame(df)
dfs.show()

# COMMAND ----------

# DBTITLE 1,Getting mapping and creating dataframe
from datetime import datetime
# making some blank files 
q1=[]
q2=[]
for firstrow in dfs.collect():
  
  first=(firstrow[0])
  timestampStr = first.strftime("%d-%b-%Y (%H:%M:%S.%f)")
  #print(timestampStr)
  
  for secondrow in dfs.collect():
    second=secondrow[0]
    #print("second:", first , secondrow[0])
    q1.append(first)
    q2.append(second)
    
    
  #datas.append(timestampStr)
  #print(type(timestampStr))
  

# COMMAND ----------

# DBTITLE 1,dataframe for only date
new_data ={"from": q1
       ,"to": q2}
df = pd.DataFrame(new_data)
dataframe = sc.createDataFrame(df)

dataframe.show()

# COMMAND ----------

# DBTITLE 1,actual creation of dataframe and taking data
from pyspark.sql.functions import countDistinct, lit
from datetime import datetime
# making some blank files 
q1=[]
q2=[]
counted=[]
for firstrow in dfs.collect():
  
  first=(firstrow[0])
  timestampStr = first.strftime("%d-%b-%Y (%H:%M:%S.%f)")
  #print(timestampStr)
  
  for secondrow in dfs.collect():
    second=secondrow[0]
    #print("second:", a , secondrow[0])
    q1.append(first)
    q2.append(second)
    
    #dfs.filter(dfs['date'] == second).show()
    #dfs.filter(dfs["date"].lt(lit(first))).show()
    #dfs.select(dfs['user', 'date']).filter(dfs['date'] >= lit(first)).filter(dfs['date'] <= lit(second)).show()
    
    
    count=dfs.filter( (dfs['date'] >= first) & (dfs['date'] <= second) ).select(countDistinct(dfs['user']))
    [counted.append(val) for val in count.collect()]
    #print(type(count))
    #dfs.select(countDistinct(dfs['user'])).show()
    
  #datas.append(timestampStr)
  #print(type(timestampStr))
  

# COMMAND ----------

#print(counted)

# COMMAND ----------

# DBTITLE 1,Final output on dataframe with dates
new_data ={"from": q1
       ,"to": q2,
          "count": counted}
df1 = pd.DataFrame(new_data)
dataframe = spark.createDataFrame(df1)

dataframe.show()

# COMMAND ----------

# taking and looking some random output
dataframe.select(dataframe['count']).take(50)
